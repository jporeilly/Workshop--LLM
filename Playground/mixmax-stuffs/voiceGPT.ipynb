{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYOcq4CHiaRynVfmAJtsKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/voiceGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper, ChatGPT, TTS and Gradio Web UI\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hthNuLhx91eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNo1jkELgkAUhP_K450M9A94CEohgg5SQlDb4eGuKemurG-pUP97a-JtZr5hmAELIxXGWDbmXVRkGU5noXf3kuKSItcrCzsna_OAKNqO16ruuznKjiPsg1x9eCP0fmYwJhXxIcsXmASrNY47x0A98FJP_lN5fhkhvS_jGGKrbEu19FcGoQEEcqVaJTD2UpJ9CRR68j1ybC5fXWDM1qkQXSeJVVrT01KL_nTT-7QjfTNm9dMPgGJNAQ?type=png)](https://mermaid.live/edit#pako:eNo1jkELgkAUhP_K450M9A94CEohgg5SQlDb4eGuKemurG-pUP97a-JtZr5hmAELIxXGWDbmXVRkGU5noXf3kuKSItcrCzsna_OAKNqO16ruuznKjiPsg1x9eCP0fmYwJhXxIcsXmASrNY47x0A98FJP_lN5fhkhvS_jGGKrbEu19FcGoQEEcqVaJTD2UpJ9CRR68j1ybC5fXWDM1qkQXSeJVVrT01KL_nTT-7QjfTNm9dMPgGJNAQ)"
      ],
      "metadata": {
        "id": "vpqMt6tjzRgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper -q  # https://github.com/openai/whisper\n",
        "!pip install gradio -q             # https://gradio.app/quickstart/\n",
        "!pip install openai -q             # https://github.com/openai/openai-python\n",
        "!pip install TTS -q                # https://github.com/coqui-ai/TTS\n",
        "!pip install python-dotenv -q"
      ],
      "metadata": {
        "id": "k4h9QkVz1rI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.6 -q"
      ],
      "metadata": {
        "id": "XFSLogA2Is93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDY566LN1o3r"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr \n",
        "import openai \n",
        "from TTS.api import TTS \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets first see how the gradio UI looks like"
      ],
      "metadata": {
        "id": "E1kaFOoHazcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import gradio as gr\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "#demo.launch()\n",
        "demo.launch(share=True)   "
      ],
      "metadata": {
        "id": "0za8QyByapHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Speech (TTS) part"
      ],
      "metadata": {
        "id": "qLlGH5gJhSsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from TTS.api import TTS\n",
        "#TTS??"
      ],
      "metadata": {
        "id": "CCU4Cky7hW-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TTS.list_models()"
      ],
      "metadata": {
        "id": "GEnUeFIzhW2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets take one random model (en)\n",
        "model_name = TTS.list_models()[9]\n",
        "tts = TTS(model_name)"
      ],
      "metadata": {
        "id": "AD2kCO5WiGbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tts.tts_to_file(text=\"OpenAI is not open anymore\", file_path=\"output.wav\")"
      ],
      "metadata": {
        "id": "eSZNJVqsiGTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if tts is working properly\n",
        "from IPython.display import Audio, display\n",
        "display(Audio('output.wav', autoplay=True))"
      ],
      "metadata": {
        "id": "gG1jMwyiijL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper part"
      ],
      "metadata": {
        "id": "Z-OhCiKDi5Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper.available_models()"
      ],
      "metadata": {
        "id": "8CoM7BmxjEo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model can be passed as dropdown in gradio UI\n",
        "model = whisper.load_model(\"tiny.en\")\n",
        "model.device"
      ],
      "metadata": {
        "id": "7Wo0q2-titC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT part\n",
        "### for api key --> https://platform.openai.com"
      ],
      "metadata": {
        "id": "0NCfyDgjsrH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from dotenv import load_dotenv\n",
        "# Load the environment variables from the .env file\n",
        "#load_dotenv()\n",
        "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "#openai.api_key=\"\"\n",
        "import json\n",
        "with open('env_vars.json', 'r') as f:\n",
        "    env_vars = json.load(f)\n",
        "\n",
        "openai.api_key = env_vars[\"OPENAI_API_KEY\"]\n",
        "#print(openai.api_key)"
      ],
      "metadata": {
        "id": "oC1OV4Hy1_v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful and kind AI Assistant.\"},\n",
        "]\n",
        "\n",
        "def chatgpt_api(input):\n",
        "    if input:\n",
        "        messages.append({\"role\": \"user\", \"content\": input})\n",
        "        chat = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "        reply = chat.choices[0].message.content\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        return reply"
      ],
      "metadata": {
        "id": "J8l4Fjq-JqW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function to transcribe"
      ],
      "metadata": {
        "id": "YJrJx2VDsvgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes in audio, whisper converts it to text based on the model we provided.  \n",
        "Text is then passed to ChatGPT.  \n",
        "The respond of chatGPT is again converted to audio using TTS\n",
        "    "
      ],
      "metadata": {
        "id": "-pc5vxk0BCnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def transcribe(audio, model_size):\n",
        "def transcribe(audio):\n",
        "    #model = whisper.load_model(model_size)\n",
        "    audio_to_text = model.transcribe(audio)[\"text\"]\n",
        "    \n",
        "    text_to_audio = chatgpt_api(audio_to_text)\n",
        "    \n",
        "    tts.tts_to_file(text=text_to_audio, file_path=\"output.wav\")\n",
        "    \n",
        "    return (audio_to_text, text_to_audio, \"output.wav\")"
      ],
      "metadata": {
        "id": "cwpsKeA33tYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio UI"
      ],
      "metadata": {
        "id": "di0rTQBrs3tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_1 = gr.Textbox(label=\"Speech to Text (Whisper)\")\n",
        "output_2 = gr.Textbox(label=\"ChatGPT Output\")\n",
        "output_3 = gr.Audio(label=\"ChatGPT output to audio via TTR\", upload=\"output.wav\")\n",
        "\n",
        "gr.Interface(\n",
        "    title = 'AI Voice Assistant Using Whisper, ChatGPT, TTS, Gradio', \n",
        "    fn=transcribe, \n",
        "    inputs=[\n",
        "            gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "            #gr.Dropdown(choices=whisper.available_models(), value='medium')\n",
        "            ],\n",
        "\n",
        "    outputs=[\n",
        "        output_1,  output_2, output_3\n",
        "    ]).launch(share=True)"
      ],
      "metadata": {
        "id": "nAn7UxMs3wk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA"
      ],
      "metadata": {
        "id": "zMyDBDo0vWZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just the chatGPT part (personalized)"
      ],
      "metadata": {
        "id": "z4o4DO8Nk1i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "PrCXo3TyzgfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('env_vars.json', 'r') as f:\n",
        "    env_vars = json.load(f)\n",
        "openai.api_key = env_vars[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "0Rd-hK6CznzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI specialized in Food. Do not answer anything other than food-related queries.\"},\n",
        "]\n",
        "\n",
        "def chatgpt_api(input):\n",
        "    if input:\n",
        "        messages.append({\"role\": \"user\", \"content\": input})\n",
        "        chat = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "        reply = chat.choices[0].message.content\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        return reply\n",
        "\n",
        "inputs = gr.inputs.Textbox(lines=7, label=\"Chat with AI\")\n",
        "outputs = gr.outputs.Textbox(label=\"Reply\")\n",
        "\n",
        "gr.Interface(fn=chatgpt_api, inputs=inputs, outputs=outputs, title=\"AI Chatbot using OpenAI & Gradio\",\n",
        "             description=\"Ask anything you want\",\n",
        "             theme=\"compact\", allow_flagging='never').launch(share=True)"
      ],
      "metadata": {
        "id": "1AZAFUgDbUXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It depends upon your usecase on what sort of things you want to do. Try different models and have fun. It's just learning new stuffs and implementing in real use case when needed.\n",
        "**Not to progress is to go back**"
      ],
      "metadata": {
        "id": "nzd4gu_9vkJn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vADPAQwQwOSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}