{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkBfMdxU6wBk+R20FhXA0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/huggingface_crash_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface Crash Course"
      ],
      "metadata": {
        "id": "oS6n9L-JHWco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necesarry libraries"
      ],
      "metadata": {
        "id": "QnXdBk6NHeQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IazpsVFiHVZY"
      },
      "outputs": [],
      "source": [
        "%%capture capturet \n",
        "#https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#capturet.show()"
      ],
      "metadata": {
        "id": "Z4yY7_skF665"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "- https://huggingface.co/docs/transformers/main_classes/pipelines"
      ],
      "metadata": {
        "id": "GVGz2uEKH-Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "BNM-FcwfH2xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('sentiment-analysis') #we put sentiment-analysis task in this case\n",
        "result = classifier(\"I enjoy sunny weather.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "MalduT9SICqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It seems its easy, right ? ðŸ˜„ Behind the scene, pipeline is handling the heavy lifting for us. \n",
        "- It first does the prepreocessing, apply a tokenizer.\n",
        "- Then feeds the pre-processed text to the model, in our case `distilbert-base-uncased-finetuned-sst-2-english`.\n",
        "- Finally, does the post-porcessing, meaning that the output must be what we want. In our case, it is sentiment analysis, so negative or positive."
      ],
      "metadata": {
        "id": "jtzs1vHoI2I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizers / Models\n",
        "- Grab any model: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "7q6-pCVnKJU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets use the auto stuffs, so it will update the rest of the code when you change the model name.\n",
        "# Its becoz, different models have their own tokenizer.\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "#from transformers import BertForSequenceClassification\n",
        "\n",
        "# lets use some different model from model hub\n",
        "model_name = \"bert-base-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer) \n",
        "result = classifier(\"I enjoy sunny weather.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "X6pPCm8aIQjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So what is tokenizer ?\n",
        "- A tokenizer is in charge of preparing the inputs for a model.\n",
        "- https://huggingface.co/docs/transformers/main_classes/tokenizer"
      ],
      "metadata": {
        "id": "xSi2ZPZAfGMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple example\n",
        "text = \"I enjoy sunny weather.\"\n",
        "tokenized_format = tokenizer(text)\n",
        "print(tokenized_format)"
      ],
      "metadata": {
        "id": "FduAqZmEdnqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The 'input_ids' list contains a sequence of integers representing the tokenized text. Each integer corresponds to a specific token in the vocabulary of the language model. In this case, the tokenized text consists of five tokens, with token IDs [146, 5548, 21162, 4250].\n",
        "\n",
        "- The 'token_type_ids' list indicates which tokens belong to the first sentence or the second sentence (in case of sentence pair tasks). Since there is only one sentence in this example, all the values in the list are zero.\n",
        "\n",
        "- The 'attention_mask' list is used to indicate which tokens in the 'input_ids' list should be attended to by the model. In this case, all tokens have a value of 1, indicating that they should all be attended to."
      ],
      "metadata": {
        "id": "gjUDPrstiElg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## lets dig little bit more \n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "back_to_token = tokenizer.convert_ids_to_tokens(ids)\n",
        "print(back_to_token)\n",
        "text_back = tokenizer.decode(ids)\n",
        "print(text_back)"
      ],
      "metadata": {
        "id": "IyHTrVQXfuDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You can implement with different ML / DL libraries (pytorch, tensorflow, jax) but lets go how we can interact with the help of LangChain.\n",
        "- Finetuning a pretrained model with pytorch: https://huggingface.co/docs/transformers/training\n",
        "- Use huggingface ecosystem with Langchain: https://python.langchain.com/en/latest/ecosystem/huggingface.html"
      ],
      "metadata": {
        "id": "qmVXsAulleJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface Hub with LangChain [link](https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_hub.html)\n",
        "- There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: `text2text-generation`, `text-generation`."
      ],
      "metadata": {
        "id": "ksRdjSSJpTRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install huggingface_hub langchain transformers"
      ],
      "metadata": {
        "id": "Jm9M4esJg6VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get tokens --> https://huggingface.co/settings/tokens\n",
        "# https://docs.python.org/3.8/library/getpass.html\n",
        "from getpass import getpass\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "id": "FS7z19impblW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "hoUvAjO1qDHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface Hub"
      ],
      "metadata": {
        "id": "QY-M1ejPu-Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "repo_id = \"google/flan-t5-xl\"\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})"
      ],
      "metadata": {
        "id": "_uqnmaGcqJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "O3tKs3j1qbwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface Local Pipelines [link](https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_pipelines.html#integrate-the-model-in-an-llmchain)"
      ],
      "metadata": {
        "id": "6wtaX3txvB8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "id": "H1Y3j98m_FXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "#model_id = \"bigscience/bloom-1b7\" # https://huggingface.co/bigscience/bloom-1b7\n",
        "model_id = \"facebook/opt-350m\"\n",
        "llm = HuggingFacePipeline.from_model_id(model_id=model_id, task=\"text-generation\", model_kwargs={\"temperature\":0.1, \"max_length\":64})"
      ],
      "metadata": {
        "id": "iScN66dHqSlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ym22TDQeD3pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "4bSHDWXRrVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_xNGHn0C1nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}