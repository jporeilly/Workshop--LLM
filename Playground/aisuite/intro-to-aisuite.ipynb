{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to Aisuite github page](https://github.com/andrewyng/aisuite/tree/main)\n",
    "\n",
    "For this example, I will be using Ollama. Please install Ollama if you don't have already.  \n",
    "- [Official website link](https://ollama.com/)\n",
    "- [Playlist of Ollama](https://youtube.com/playlist?list=PLz-qytj7eIWX-bpcRtvkixvo9fuejVr8y&si=S4sHAFkzLVleuuIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Check if Ollama is installed, type `ollama` in your terminal.\n",
    "- If you run it manually, no need to run `ollama serve` as it will be already running.\n",
    "- If not triggered manually, run `ollama serve` in the terminal.\n",
    "\n",
    "Once you run, check it is listeninig in the port it should be [running](http://127.0.0.1:11434)\n",
    "\n",
    "- Model via Ollama locally, `ollama run <model_of_your_choice>`\n",
    "- Run `ollama list` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I assist you?\n"
     ]
    }
   ],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "# Initialize the AI client for accessing the language model\n",
    "client = ai.Client()\n",
    "\n",
    "# Define a conversation with a system message and a user message\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
    "    {\"role\": \"user\", \"content\": 'Hi'},\n",
    "]\n",
    "\n",
    "# Request a response from the model\n",
    "response = client.chat.completions.create(model=\"ollama:llama3.1:8b\", messages=messages)\n",
    "\n",
    "# Print the model's response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define a function to interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(message, sys_message=\"You are a helpful agent.\", model=\"ollama:llama3.1:8b\"):\n",
    "    # Initialize the AI client for accessing the language model\n",
    "    client = ai.Client()\n",
    "\n",
    "    # Construct the messages list for the chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_message},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "\n",
    "    # Send the messages to the model and get the response\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    # Return the content of the model's response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Nepal is Kathmandu! Would you like to know more about Nepal or is there something else I can help you with?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('Hello, what is the capital of Nepal ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pretty Printing Function\n",
    "Let's define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "# Set a custom width for pretty-printing\n",
    "def pprint(data, width=80):\n",
    "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
    "    pp(data, width=width)# List of model identifiers to query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query multiple models for a common Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('llama3.1:8b: \\n'\n",
      " ' The concept of Artificial Intelligence (AI) originated in the mid-20th '\n",
      " 'century with the development of the Dartmouth Summer Research Project on '\n",
      " 'Artificial Intelligence in 1956, led by computer scientists John McCarthy, '\n",
      " 'Marvin Minsky, Nathaniel Rochester, and Claude Shannon. ')\n",
      "('hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:Q5_K_S: \\n'\n",
      " ' The concept of Artificial Intelligence (AI) has its roots in ancient Greek '\n",
      " \"philosophy, particularly Plato's idea of automaton, and was first explored \"\n",
      " 'scientifically by pioneers such as Charles Babbage and Alan Turing in the '\n",
      " '19th century. ')\n",
      "('phi3:latest: \\n'\n",
      " ' Artificial intelligence originated in the mid-20th century as an attempt to '\n",
      " 'create machines that could mimic human cognitive functions such as learning '\n",
      " 'and problem solving. ')\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    'llama3.1:8b',\n",
    "    'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:Q5_K_S',\n",
    "    'phi3:latest'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each model\n",
    "ret = []\n",
    "\n",
    "# Loop through each model and get a response for the specified question\n",
    "for x in models:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'ollama:{x}'))\n",
    "\n",
    "# Print the model's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(models[idx] + ': \\n ' + x + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Different AI Providers for a Common Question\n",
    "- Here lets try with groq and Ollama, you can use other providers if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets grab the groq's api key first https://console.groq.com/playground\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loop through each provider and get a response for the specified question\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m providers:\n\u001b[0;32m---> 14\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWrite a short one sentence explanation of the origins of AI?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the provider's name and its corresponding response\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ret):\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mask\u001b[0;34m(message, sys_message, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sys_message},\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Send the messages to the model and get the response\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Return the content of the model's response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/client.py:108\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders:\n\u001b[1;32m    107\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mprovider_configs\u001b[38;5;241m.\u001b[39mget(provider_key, {})\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders[provider_key] \u001b[38;5;241m=\u001b[39m \u001b[43mProviderFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders\u001b[38;5;241m.\u001b[39mget(provider_key)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m provider:\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/provider.py:46\u001b[0m, in \u001b[0;36mProviderFactory.create_provider\u001b[0;34m(cls, provider_key, config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Instantiate the provider class\u001b[39;00m\n\u001b[1;32m     45\u001b[0m provider_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, provider_class_name)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/providers/groq_provider.py:19\u001b[0m, in \u001b[0;36mGroqProvider.__init__\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m API key is missing. Please provide it in the config or set the GROQ_API_KEY environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mgroq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/groq/_client.py:99\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.groq.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mEmbeddings(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/groq/_base_client.py:824\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m     )\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    812\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    813\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    823\u001b[0m )\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/groq/_base_client.py:722\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    721\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "# List of AI model providers to query\n",
    "providers = [\n",
    "    'groq:llama-3-8b-8192',\n",
    "    'ollama:llama3.1:8b'\n",
    "    #'openai:gpt-4o',\n",
    "    #'anthropic:claude-3-5-sonnet-20240620'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each provider\n",
    "ret = []\n",
    "\n",
    "# Loop through each provider and get a response for the specified question\n",
    "for x in providers:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
    "\n",
    "# Print the provider's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets grab the groq's api key first https://platform.openai.com/settings/organization/api-keys\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LLMError",
     "evalue": "Ollama request failed: Client error '404 Not Found' for url 'http://localhost:11434/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/providers/ollama_provider.py:46\u001b[0m, in \u001b[0;36mOllamaProvider.chat_completions_create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     response \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_CHAT_COMPLETION_ENDPOINT,\n\u001b[1;32m     43\u001b[0m         json\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     44\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:  \u001b[38;5;66;03m# Handle connection errors\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'http://localhost:11434/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLLMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loop through each provider and get a response for the specified question\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m providers:\n\u001b[0;32m---> 14\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWrite a short one sentence explanation of the origins of AI?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the provider's name and its corresponding response\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ret):\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mask\u001b[0;34m(message, sys_message, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sys_message},\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Send the messages to the model and get the response\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Return the content of the model's response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/client.py:117\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load provider for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Delegate the chat completion to the correct provider's implementation\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/yt-code/youtube-stuffs/aisuite/.venv/lib/python3.11/site-packages/aisuite/providers/ollama_provider.py:50\u001b[0m, in \u001b[0;36mOllamaProvider.chat_completions_create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_CONNECT_ERROR_MESSAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m http_err:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama request failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLLMError\u001b[0m: Ollama request failed: Client error '404 Not Found' for url 'http://localhost:11434/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404"
     ]
    }
   ],
   "source": [
    "# List of AI model providers to query\n",
    "providers = [\n",
    "    #'groq:llama-3-8b-8192',\n",
    "    'ollama:llama3.1:8b'\n",
    "    'openai:gpt-4o',\n",
    "    #'anthropic:claude-3-5-sonnet-20240620'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each provider\n",
    "ret = []\n",
    "\n",
    "# Loop through each provider and get a response for the specified question\n",
    "for x in providers:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
    "\n",
    "# Print the provider's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('openai:gpt-4o: \\n'\n",
      " 'The origins of AI trace back to mid-20th century theoretical discussions and '\n",
      " 'the development of early computational models aimed at simulating human '\n",
      " 'intelligence, notably marked by the 1956 Dartmouth Conference where the term '\n",
      " '\"artificial intelligence\" was first coined. \\n'\n",
      " '\\n')\n",
      "('ollama:llama3.1:8b: \\n'\n",
      " 'The concept of artificial intelligence (AI) dates back to ancient Greece, '\n",
      " 'but modern AI research began in the 1950s with the work of computer '\n",
      " 'scientists such as Alan Turing and Marvin Minsky, who proposed the idea of '\n",
      " 'creating machines that could think and learn like humans. \\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "# List of AI model providers to query\n",
    "providers = [\n",
    "    #'groq:llama-3-8b-8192',\n",
    "    'openai:gpt-4o',\n",
    "    'ollama:llama3.1:8b'\n",
    "    #'anthropic:claude-3-5-sonnet-20240620'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each provider\n",
    "ret = []\n",
    "\n",
    "# Loop through each provider and get a response for the specified question\n",
    "for x in providers:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
    "\n",
    "# Print the provider's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the issues section in github for more info, https://github.com/andrewyng/aisuite/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlclusion\n",
    "- makes it easy for developers to use large language models from multiple providers\n",
    "- lets you pick a \"provider:model\" just by changing one string, like openai:gpt-4o, anthropic:claude-3-5-sonnet-20241022, ollama:llama3.1:8b, etc.\n",
    "\n",
    "#### Is there something like this already, Yes, check this [link](https://github.com/BerriAI/litellm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
